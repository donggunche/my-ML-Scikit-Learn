{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-70fd8ddc4cb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), \n\u001b[0;32m     10\u001b[0m   ('svc', svm_clf)],voting='hard') \n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mvoting_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.ensemble import RandomForestClassifier \n",
    ">>> from sklearn.ensemble import VotingClassifier \n",
    ">>> from sklearn.linear_model import LogisticRegression \n",
    ">>> from sklearn.svm import SVC\n",
    ">>> from sklearn.datasets import make_moons\n",
    ">>> log_clf = LogisticRegression() \n",
    ">>> rnd_clf = RandomForestClassifier() \n",
    ">>> svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=400, noise=0.1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), \n",
    ">>>   ('svc', svm_clf)],voting='hard') \n",
    ">>> voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import accuracy_score \n",
    ">>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf): \n",
    ">>>     clf.fit(X_train, y_train) \n",
    ">>>     y_pred = clf.predict(X_test) \n",
    ">>>     print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), \n",
    ">>>   ('svc', svm_clf)],voting='soft') \n",
    ">>> voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.metrics import accuracy_score \n",
    ">>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf): \n",
    ">>>     clf.fit(X_train, y_train) \n",
    ">>>     y_pred = clf.predict(X_test) \n",
    ">>>     print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,        \n",
    "max_samples=100, bootstrap=True, n_jobs=-1) \n",
    "bag_clf.fit(X_train, y_train) \n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857142857142858"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)\n",
    ">>> bag_clf.fit(X_train, y_train) \n",
    ">>> bag_clf.oob_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.metrics import accuracy_score \n",
    ">>> y_pred = bag_clf.predict(X_test) \n",
    ">>> accuracy_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.ensemble import RandomForestClassifier\n",
    ">>> rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) \n",
    ">>> rnd_clf.fit(X_train, y_train)\n",
    ">>> y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> bag_clf.fit(X_train, y_train)\n",
    ">>> y_pred_bag = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08955767670032165\n",
      "sepal width (cm) 0.02184529356009138\n",
      "petal length (cm) 0.45682150494849116\n",
      "petal width (cm) 0.43177552479109577\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.datasets import load_iris \n",
    ">>> iris = load_iris() \n",
    ">>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1) \n",
    ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\"]) \n",
    ">>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_): \n",
    ">>>     print(name, score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=1.0, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
    ">>> gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) \n",
    ">>> gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=119,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import numpy as np \n",
    ">>> from sklearn.model_selection import train_test_split\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    "\n",
    ">>> X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    ">>> gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) \n",
    ">>> gbrt.fit(X_train, y_train)\n",
    ">>> errors = [mean_squared_error(y_val, y_pred)         \n",
    "      for y_pred in gbrt.staged_predict(X_val)] \n",
    ">>> bst_n_estimators = np.argmin(errors)\n",
    ">>> gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) \n",
    ">>> gbrt_best.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
